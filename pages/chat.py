"""
| streamlit - multipage app - page1_chat |
--------
ç›®æ¨™ï¼šå¯¦åšä¸€å€‹ä¾›èŠå¤©çš„ä»‹é¢
--------
ç´€éŒ„ï¼š
2023.11.20 - ä¹‹å¾Œå¯ä»¥å˜—è©¦åªä¿ç•™å‹•è©ã€å½¢å®¹è©ã€åè©å† embedding
2023.11.20 - å®Œæˆï¼Œä½†æŸ¥ä¸æº–ï¼Œå¾…ç¢ºèªåŸå› 
--------
åƒè€ƒï¼š
https://zhuanlan.zhihu.com/p/46016518
https://python.langchain.com/docs/use_cases/question_answering/
https://python.langchain.com/docs/integrations/vectorstores/chroma
https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L12-v2
https://clusteredbytes.pages.dev/posts/2023/langchain-parent-document-retriever/
vector embeddings : 1. feature engineering 2. neural network
chroma : å°ˆé–€ç”¨ä¾†å­˜ embedding çµæœçš„å‘é‡è³‡æ–™åº«
"""

### è¼‰å…¥å¥—ä»¶
import os
import sys
import time
import streamlit as st
from app import authenticator
from streamlit_extras.switch_page_button import switch_page
sys.path.insert(0, "..") # å‘Šè¨´ python å¾çˆ¶è³‡æ–™å¤¾é–‹å§‹å¾€ä¸‹æ‰¾
from dotenv import load_dotenv
from QAflow.parse_data import parse_data
from QAflow.lda_clustering import lda_clustering
from QAflow.build_index import build_index
from QAflow.retrieve_data import retrieve_data

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()
indexing_folder = os.getenv("indexing_folder")
# è©åµŒå…¥æ¨¡å‹åç¨±
model_name = os.getenv("model_name")
# chroma è³‡æ–™å¤¾
chroma_dir = os.getenv("chroma_dir")
# data è³‡æ–™å¤¾
data_dir = os.getenv("data_dir")
# embeddings åˆ‡ chunk åƒæ•¸
child_chunk_size = int(os.getenv("child_chunk_size"))
child_chunk_overlap = int(os.getenv("child_chunk_overlap"))
# lda ä¸»é¡Œæ•¸
n_topics = int(os.getenv("n_topics"))

if st.session_state["logout"]:
    switch_page("é¦–é ")
else:
    # ----- HIDE STREAMLIT STYLE -----
    hide_st_style = """
                <style>
                #MainMenu {visibility: hidden;}
                footer {visibility: hidden;}
                header {visibility: hidden;}
                </style>
                """
    st.markdown(hide_st_style, unsafe_allow_html=True)
    authenticator.logout("Logout", "sidebar", key='search')
    ### å¤§æ¨™é¡Œ
    st.title("å¸¸è¦‹æ°£è±¡å•ç­”")
    ### åˆå§‹åŒ– messages å…±ç”¨è®Šæ•¸
    if "messages" not in st.session_state:
        st.session_state.messages = []
    ### ä¿å­˜å°è©±ç´€éŒ„
    for message in st.session_state.messages:
        with st.chat_message(name=message["role"], avatar=message["avatar"]):
            st.markdown(message["content"], unsafe_allow_html=True)
    ### å°è©±å€ï¼ˆ:= ç‚ºè±¡ç‰™é‹ç®—å­ï¼‰
    if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œï¼"):
        st.session_state.messages.append({"role": "user", "avatar": "ğŸ‘¨â€ğŸ’»", "content": prompt})
        # ä½¿ç”¨è€…æå•
        with st.chat_message(name="user", avatar="ğŸ‘¨â€ğŸ’»"):
            st.markdown(prompt)
        # æ©Ÿå™¨äººå›ç­”
        with st.chat_message(name="assistant", avatar="ğŸŒ„"):
            message_placeholder = st.empty()
            time.sleep(0.01)
            ts = time.time()
            with st.spinner("please wait for the answer..."):    
                # è§£æè³‡æ–™
                parser = parse_data()
                docs = []
                for filename in os.listdir(indexing_folder):
                    doc = parser.parse_pdf(filepath=f"{indexing_folder}/{filename}")
                    docs.extend(doc)
                # ä¸»é¡Œåˆ†é¡
                lda = lda_clustering()
                if (os.path.exists(chroma_dir)) & (os.path.exists(data_dir)):
                    pass
                else:
                    # è¨“ç·´åˆ†é¡
                    topics = lda.train(original_docs=docs, n_topics=n_topics)
                    # å»ºç«‹ç´¢å¼•
                    for (topic, all_qa_list) in topics.items():
                        boolean = build_index(topic=topic,
                                            all_qa_list=all_qa_list,
                                            model_name=model_name,
                                            chroma_dir=chroma_dir,
                                            data_dir=data_dir,
                                            child_chunk_size=child_chunk_size,
                                            child_chunk_overlap=child_chunk_overlap)
                # å–å¾—å•é¡Œåˆ†é¡
                query_topic = lda.predict(query=prompt,
                                        docs=docs,
                                        n_topics=n_topics)
                # å›å‚³ç­”æ¡ˆ
                full_response = retrieve_data(query=prompt,
                                            topic=query_topic,
                                            model_name=model_name,
                                            chroma_dir=chroma_dir,
                                            data_dir=data_dir,
                                            child_chunk_size=child_chunk_size,
                                            child_chunk_overlap=child_chunk_overlap)
                te = time.time()
                elapsed_time = f"<br><span style='font-family:cursive;font-size:14px;color:#6B8E3;'>time spent = {round(te-ts, 2)} seconds.</span>"
                message_placeholder.markdown(full_response + elapsed_time, unsafe_allow_html=True)
        st.session_state.messages.append({"role": "assistant", "avatar": "ğŸŒ„", "content": full_response})
